[ { "title": "Git for Beginners: Basics and Essential Commands (2/3)", "url": "/posts/git-part-2/", "categories": "git-series, vcs, git", "tags": "git, vcs, git-series, version control, dvcs", "date": "2026-01-14 00:00:00 +0530", "content": "… The dependency didn’t last. BitKeeper was no longer an option for Linux, previously called Freax. At that point, the project had already outgrown patches, email, and centralized version control. Going back wasn’t possible So Linus Torvalds, the developer behind Linux, did the only reasonable thing left. He wrote a tool that solved the version control problems and called it Git With this context, Git starts to make more sense. Its design centers on scalability rather than immediate intuitiveness. Common Beginner Misconceptions Before touching a single command, we need to clear a few ideas that almost everyone gets wrong when starting with Git. Git is GitHub Git != Github It “tracks changes” locally Git can have multiple remotes where you can store your repo Github is just a remote managed by Microsoft similar to Gitlab and Bitbucket managed by other corps You can host your own remote using software like Forgejo, Gitlab CE, Gitea Git automatically tracks my changes You explicitly tell Git to track certain Files When you want to “save the current state”, you make a commit A commit is a backup Commits are just like a checkpoint which you create Commits are manually pushed to remote Branches are copies of folders Branches are just a pointer to a commit The commit a branch points to is called the tip of the branch Since branches are just pointers, creating them is cheap and fast This is why people say: “branches are cheap in Git” Git Basics, Workflow and Essential Commands What Git Calls a Repository? A repository (often shortened to repo) is a project tracked by Git. Essentially, it’s just a directory which contains your project files But how does can I identify a git repo? It is identified by a hidden .git directory If a directory has a .git subdirectory inside it, Git considers it a repository. The files you edit live in the working directory. The history and metadata live inside the aforementioned .git subdirectory Most Git commands are simply moving changes between these two spaces. Git Config Git uses configuration files to record who made a change and when OR in more technically correct words: Git uses configuration files to determine the author identity and timestamps recorded in commits. Well, where is this config file? There are a multitude of config files which Git looks for by default. Each config file has a Scope or Level. These levels decide where a setting applies and which setting wins if there is a conflict. Scope Location Applies to System /etc/gitconfig Every User Global ~/.gitconfig Current User Local .git/config Specific Repo Worktree .git/config.worktree Worktree Specific This neat diagram by Boot.dev explains really well which config file overrides which one: Credit: Boot.dev You don’t need to think much about the Git config file at this point of time. Most of the time you are going to use the Global config file Substitute the e-mail and username with your own and run these commands to setup a basic Git config git config --global user.email \"your-email@example.com\" git config --global user.name \"Your Name\" Workflow At a high level, Git workflow looks something like this: We’ll walk through this loop step by step. Working Directory This is where you edit your files, compile, run and test your project BUT at this point it’s just a simple directory. Git has no idea that it even exists. To start using this working directory as a Git repo, we need to initialize it with Git git init This create .git directory in your working directory. At this point Git is NOT tracking any files, it simply exists there ready to be commanded! Check the Current State Make it a habit to run this whenever you start working in a Git repo git status Running git status gives us an overview of what Git sees. A file in a Git repo can have three states: Untracked: Not tracked by Git Staged: File is tracked by Git and is will be included in next commit Committed: Saved to repo history Staged files are said to be in “Staging Area” also referred as the “Index” git status also gives you an overview of which files have changed in the repo. Using this information we can move onto the next step Choosing What Belongs Together Good News! You don’t need to record all the changes across files in a single go. We can add the desired files by specifying file names either one-by-one, by-directory, or glob patterns. These files will be added to the staging area git add file.md To add all standard files and directories in current working directory, do git add . Recording a Snapshot Once you’re satisfied with what you’ve selected, you record it. git commit -m \"Describe what changed\" This creates a commit in our Git history. A commit is associated with a long, unique identifier that represents the commit’s contents and metadata called a Hash. Commit hash is derived using various sources such as: content changes commit message Author Name Author e-mail Date and Time Parent commit hash If any of these change, the resulting hash changes as well. Viewing History To perform any operation on or using our history, we need to first be able to see what happened in the past. Git log command allows us to do just that git log This shows you: List of commits Commit messages Authors Order in which commits were made (or reverted) If you just want to see the commit hash and commit message, you can run git log --oneline Conclusion At this stage, you know how to work with git and these are the commands which Developers and DevOps creatures use majority of the time. If you’re stuck with git, try this site In the next post we’ll look inside the .git directory and answer questions like: Where commits actually live What branches really point to How Git stores history on disk Understanding that internal structure is how you become the Git Master!" }, { "title": "Why Version Control Exists: Before Git (1/3)", "url": "/posts/git-part-1/", "categories": "git-series, vcs, git", "tags": "git, vcs, git-series, version control, dvcs", "date": "2026-01-13 23:00:00 +0530", "content": "Subject: Re: build failing on server Hi all, Does anyone still have parser.c from last Thursday’s build? I’m looking for the version prior to the logging changes. The build was stable at that point, and I suspect a regression after that. If someone has it, please send the file. Thanks, a tired developer, 1992 That might have felt like a strange mail, but it was routine to the developers who lived through it This was exactly the kind of friction that forced better tools into existence. To understand why, we need to go back to a time when computers were still strange boxes that took their time “thinking” Mail and Pendrive Era Picture yourself, a young enthusiastic nerd in the 90s, sitting in front of your IBM PC clone running Minix. You and you friends are working on a fun project Freax You know a guy who really knows his way around MASM so you drop a mail asking for help. He is onboard, you send him the code zip over mail and so the collaboration begins. Life goes on, your project is being talked about on IRC Channels and before anyone knows, tens of people are sending over their patches for this revolutionary new software You send latest.zip to Dennis Dennis reverts with an updated parser.c You manually pick and choose changes to keep, compile and run Guido is joining as a new contributor so Tim sends him project.zip You notify Tim about the updated parser, send him the latest_really.zip Here comes Bjarne wholly persuaded by Richard’s take on Libre Licensing and now we suddenly have I_Love_Libre.zip The issue is glaringly obvious. THERE IS NO ORGANIZATION IN THE CODEBASE Everyone has their own version of the truth. This is where early version control systems enter the picture. Early Version Control Systems Centralized VCS There is one central repository that everyone depends on (Source of Truth) You need to be connected to the server in order to make changes Each developer gets a working copy, not the full repository History is on central server CVS There is chatter about version control and there exists this software named CVS. You start using CVS with the help of your friend Ari Now you have one server hosting the Project. Workflow looks something like this: Copy project from Server cvs checkout project Everyone has a local copy on your computer BUT meaningful actions still depend on the server You start modifying main.c on your local FS. There is no lock on files. Someone changed main.c while you were working on it and committed before you? Well, tough luck Update before committing cvs update This the stage where you pull changes from the server (yea change first, pull later) Remember your main.c changes? They are currently in conflict with latest at server. You have to manually pick and choose changes to keep Commit cvs commit After resolving conflicts, you commit your changes. Each file is committed independently - if something fails midway, you can end up with a partially applied change. SVN Someone on the IRC suggested Subversion so you decides to give it a try! The workflow stayed similar Copy project from Server svn checkout project Lock file if you want svn lock main.c -m \"Editing civilization\" You can edit the file locally. Others can still modify it locally, but they cannot commit changes while the lock is held. Commit svn commit main.c -m \"Update civilization\" Now you can commit changes atomically. Some improvement finally! Unlock the File svn unlock main.c The file is now available for someone else Issues with Centralized VCS The server is a Single Point of Failure History is stored on the server making your working copy incomplete Branching while Technically Possible, is expensive Distributed VCS No central server (No source of Truth) Every local copy is a Full Repository including history and branches Branches are Lightweight and Cheap making it part of workflow BitKeeper Freax has become widespread. Hundreds of contributors and Thousands of lines of code CVS and SVN don’t scale to this level so you decided to go Distributed Bitkeeper is a famous DVCS but it is proprietary. Alas! the project needs it so you compromise and start using it The catch is the license. Contributors are explicitly forbidden from reverse-engineering BitKeeper, and violating this condition means losing access entirely. Latest workflow is something like this Clone the Repository bk clone bk://freax.bkbits.net/freax-2.5 freax cd freax You have a full repo copy with all history and branches Make Changes Locally Edit the files you want to change. No need to manually Lock and Unlock files. No connection to the server Commit Changes bk commit Commits were local and Atomic Pull Changes bk pull Pull changes from others. This might create conflict Resolve Conflicts BitKeeper knew which changes conflicted so conflict resolution was easier Merges had explicit Record making it traceable Push Changes bk push Your changes are now available to others. Contributors choose if they want to pull changes or not A new beginning Freax now depends on a proprietary tool, governed by a license that could be revoked at any time. For a project built on open collaboration, that dependency could not last…" }, { "title": "Homelab Part 1 - Hardware, Locations, and Design Goals", "url": "/posts/homelab/", "categories": "homelab, architecture", "tags": "servers, proxmox, homelab, hybrid cloud, architecture diagram", "date": "2026-01-06 05:30:00 +0530", "content": "I have been trying to start blogging and documenting my homelab for a long time (~2 years) but the biggest hurdle has been DIAGRAMS Finally, on this Holy Day of 6th January 2026, I present to the people on the web with my Homelab! This is going to be multi-part blog post which will be roughly divided into 5 parts where I will explain and reason: Why, Goals, Infra Overview Cloud Services Used, Workload Placement Networking, DNS &amp; Traffic Flow Security, Identity &amp; Observability Automation, Ideal Homelab Architecture Why This Homelab Exists I got into Homelabbing and Self-Hosting during 2023 when I was exploring Linux &amp; Docker. Coming across YT Channels like Techno Tim, Chris Titus Tech, IBRACORP, The Lord’s Heir, and his Deepfake made me explore Open Source Tech which eventually led to need of Selfhosting and using Libre Software Design Goals My homelab has been designed and re-invented multiple times which led to me making some Design Choices such as: Separation between prod and lab Single ingress point Minimal public attack surface Some of things which I would eventually like to perfect in this setup include: Strong identity and access control Reproducible infrastructure Avoiding Single Points of Failure Locations / Tenancies On-Prem Homelab Due to a single High end hardware and 3 low spec devices I decided to divide Prod and Test into 2 mostly-independent environments former being an Independent PVE and PBS node and latter being a PVE Cluster to learn on Host CPU RAM Environment PC Ryzen 5 5600x 48 GB DDR4 Prod Pi-4 Cortex-A72 4 GB LPDDR4 Test Acer AMD A8-7410 16 GB DDR3 Test HP i3-7020U 32 GB DDR4 Test Oracle Cloud Infrastructure (OCI) This tenancy has high Uptime by the virtue of it being “Cloud” thus it has been crowned as the INGRESS Currently I am only effective using One ARM machine but I do have plans to utilize 3 VMs here to mitigate single point of failure to an extent. Keep in tune for that blog! Instance Shape Arch Environment OCI-Bom-ARM VM.Standard.A1.Flex ARM Prod OCI-Bom-AMD-1 VM.Standard.E2.1.Micro x86 Test OCI-Bom-AMD-2 VM.Standard.E2.1.Micro x86 Test The two VMs in Test environment are ephemeral Virtualization Stack Why Proxmox Proxmox is quite a popular solution in the Selfhosting community which translates to better support. I also found it easier to get started with Proxmox with this support. Existence of PBS and PVE Helper Scripts now reinforces the decision of deploying PVE Since Proxmox is based on KVM, guides lining out Hardware Passthrough work almost 1:1 on Proxmox. Underlying is a Debian Environment which makes it easier to debug, maintain and troubleshoot for a regular Linux user like me VM vs LXC vs Docker VMs are divided based on their usage such as Media for Media Server, k3s for VMs running Kubernetes Cluster etc. VMs are first-class citizen in the what-to-use debate in my Homelab. LXCs are used when I come across hardware constraints, such as, Single GPU Passthrough, SATA Passthrough when SATA bus is shared with USB bus in the motherboard For specific service deployment such as DNS, Reverse Proxy etc, Docker is preferred unless using Docker would require additional un-necessary configuration or more importantly, additional Security Permissions which is the case when it comes to DNS Both LXCs and Docker deploy services based on stacks such as DNS Stack which consists of Pihole, Unbound and Pihole-Updatelist. This way ensures I don’t over-complicate my infra but still retain necessary segregation High-Level Architecture Explanation Let’s assume a request is sent to dash.example.com which corresponds to service: dash on Other-Services stack in Media Server at Proxmox PC On-Prem The request will only reach Traefik at OCI-Bom-Arm. Traefik will check it’s backend i.e, File and Redis. If service is found, it will redirect to Keycloak and authenticated via: Native Auth if : service supports OAuth and OAuth is setup at service Middleware Auth if: no native OAuth is setup AND oauth middleware is applied at service But how does Traefik know where Dash service is at? This is where Redis backend and Tailscale come to the picture! Traefik-Kop is a service which writes to Redis over at OCI-Bom-ARM and traffic is routed using Headscale and Outbound NAT at OPNsense Thanks! Thank you for going through this long, complicated story of my Homelab, do leave your comment, react and share this with your Tech Ninja Friends!" } ]
