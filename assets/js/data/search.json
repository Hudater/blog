[ { "title": "Why Version Control Exists: Before Git", "url": "/posts/git-part-1/", "categories": "vcs, git", "tags": "git, vcs, version control, dvcs", "date": "2026-01-13 23:00:00 +0530", "content": "Subject: Re: build failing on server Hi all, Does anyone still have parser.c from last Thursday’s build? I’m looking for the version prior to the logging changes. The build was stable at that point, and I suspect a regression after that. If someone has it, please send the file. Thanks, a tired developer, 1992 That might have felt like a strange mail, but it was routine to the developers who lived through it This was exactly the kind of friction that forced better tools into existence. To understand why, we need to go back to a time when computers were still strange boxes that took their time “thinking” Mail and Pendrive Era Picture yourself, a young enthusiastic nerd in the 90s, sitting in front of your IBM PC clone running Minix. You and you friends are working on a fun project Freax You know a guy who really knows his way around MASM so you drop a mail asking for help. He is onboard, you send him the code zip over mail and so the collaboration begins. Life goes on, your project is being talked about on IRC Channels and before anyone knows, tens of people are sending over their patches for this revolutionary new software You send latest.zip to Dennis Dennis reverts with an updated parser.c You manually pick and choose changes to keep, compile and run Guido is joining as a new contributor so Tim sends him project.zip You notify Tim about the updated parser, send him the latest_really.zip Here comes Bjarne wholly persuaded by Richard’s take on Libre Licensing and now we suddenly have I_Love_Libre.zip The issue is glaringly obvious. THERE IS NO ORGANIZATION IN THE CODEBASE Everyone has their own version of the truth. This is where early version control systems enter the picture. Early Version Control Systems Centralized VCS There is one central repository that everyone depends on (Source of Truth) You need to be connected to the server in order to make changes Each developer gets a working copy, not the full repository History is on central server CVS There is chatter about version control and there exists this software named CVS. You start using CVS with the help of your friend Ari Now you have one server hosting the Project. Workflow looks something like this: Copy project from Server cvs checkout project Everyone has a local copy on your computer BUT meaningful actions still depend on the server You start modifying main.c on your local FS. There is no lock on files. Someone changed main.c while you were working on it and committed before you? Well, tough luck Update before committing cvs update This the stage where you pull changes from the server (yea change first, pull later) Remember your main.c changes? They are currently in conflict with latest at server. You have to manually pick and choose changes to keep Commit cvs commit After resolving conflicts, you commit your changes. Each file is committed independently - if something fails midway, you can end up with a partially applied change. SVN Someone on the IRC suggested Subversion so you decides to give it a try! The workflow stayed similar Copy project from Server svn checkout project Lock file if you want svn lock main.c -m \"Editing civilization\" You can edit the file locally. Others can still modify it locally, but they cannot commit changes while the lock is held. Commit svn commit main.c -m \"Update civilization\" Now you can commit changes atomically. Some improvement finally! Unlock the File svn unlock main.c The file is now available for someone else Issues with Centralized VCS The server is a Single Point of Failure History is stored on the server making your working copy incomplete Branching while Technically Possible, is expensive Distributed VCS No central server (No source of Truth) Every local copy is a Full Repository including history and branches Branches are Lightweight and Cheap making it part of workflow BitKeeper Freax has become widespread. Hundreds of contributors and Thousands of lines of code CVS and SVN don’t scale to this level so you decided to go Distributed Bitkeeper is a famous DVCS but it is proprietary. Alas! the project needs it so you compromise and start using it The catch is the license. Contributors are explicitly forbidden from reverse-engineering BitKeeper, and violating this condition means losing access entirely. Latest workflow is something like this Clone the Repository bk clone bk://freax.bkbits.net/freax-2.5 freax cd freax You have a full repo copy with all history and branches Make Changes Locally Edit the files you want to change. No need to manually Lock and Unlock files. No connection to the server Commit Changes bk commit Commits were local and Atomic Pull Changes bk pull Pull changes from others. This might create conflict Resolve Conflicts BitKeeper knew which changes conflicted so conflict resolution was easier Merges had explicit Record making it traceable Push Changes bk push Your changes are now available to others. Contributors choose if they want to pull changes or not A new beginning Freax now depends on a proprietary tool, governed by a license that could be revoked at any time. For a project built on open collaboration, that dependency could not last." }, { "title": "Homelab Part 1 - Hardware, Locations, and Design Goals", "url": "/posts/homelab/", "categories": "homelab, architecture", "tags": "servers, proxmox, homelab, hybrid cloud, architecture diagram", "date": "2026-01-06 05:30:00 +0530", "content": "I have been trying to start blogging and documenting my homelab for a long time (~2 years) but the biggest hurdle has been DIAGRAMS Finally, on this Holy Day of 6th January 2026, I present to the people on the web with my Homelab! This is going to be multi-part blog post which will be roughly divided into 5 parts where I will explain and reason: Why, Goals, Infra Overview Cloud Services Used, Workload Placement Networking, DNS &amp; Traffic Flow Security, Identity &amp; Observability Automation, Ideal Homelab Architecture Why This Homelab Exists I got into Homelabbing and Self-Hosting during 2023 when I was exploring Linux &amp; Docker. Coming across YT Channels like Techno Tim, Chris Titus Tech, IBRACORP, The Lord’s Heir, and his Deepfake made me explore Open Source Tech which eventually led to need of Selfhosting and using Libre Software Design Goals My homelab has been designed and re-invented multiple times which led to me making some Design Choices such as: Separation between prod and lab Single ingress point Minimal public attack surface Some of things which I would eventually like to perfect in this setup include: Strong identity and access control Reproducible infrastructure Avoiding Single Points of Failure Locations / Tenancies On-Prem Homelab Due to a single High end hardware and 3 low spec devices I decided to divide Prod and Test into 2 mostly-independent environments former being an Independent PVE and PBS node and latter being a PVE Cluster to learn on Host CPU RAM Environment PC Ryzen 5 5600x 48 GB DDR4 Prod Pi-4 Cortex-A72 4 GB LPDDR4 Test Acer AMD A8-7410 16 GB DDR3 Test HP i3-7020U 32 GB DDR4 Test Oracle Cloud Infrastructure (OCI) This tenancy has high Uptime by the virtue of it being “Cloud” thus it has been crowned as the INGRESS Currently I am only effective using One ARM machine but I do have plans to utilize 3 VMs here to mitigate single point of failure to an extent. Keep in tune for that blog! Instance Shape Arch Environment OCI-Bom-ARM VM.Standard.A1.Flex ARM Prod OCI-Bom-AMD-1 VM.Standard.E2.1.Micro x86 Test OCI-Bom-AMD-2 VM.Standard.E2.1.Micro x86 Test The two VMs in Test environment are ephemeral Virtualization Stack Why Proxmox Proxmox is quite a popular solution in the Selfhosting community which translates to better support. I also found it easier to get started with Proxmox with this support. Existence of PBS and PVE Helper Scripts now reinforces the decision of deploying PVE Since Proxmox is based on KVM, guides lining out Hardware Passthrough work almost 1:1 on Proxmox. Underlying is a Debian Environment which makes it easier to debug, maintain and troubleshoot for a regular Linux user like me VM vs LXC vs Docker VMs are divided based on their usage such as Media for Media Server, k3s for VMs running Kubernetes Cluster etc. VMs are first-class citizen in the what-to-use debate in my Homelab. LXCs are used when I come across hardware constraints, such as, Single GPU Passthrough, SATA Passthrough when SATA bus is shared with USB bus in the motherboard For specific service deployment such as DNS, Reverse Proxy etc, Docker is preferred unless using Docker would require additional un-necessary configuration or more importantly, additional Security Permissions which is the case when it comes to DNS Both LXCs and Docker deploy services based on stacks such as DNS Stack which consists of Pihole, Unbound and Pihole-Updatelist. This way ensures I don’t over-complicate my infra but still retain necessary segregation High-Level Architecture Explanation Let’s assume a request is sent to dash.example.com which corresponds to service: dash on Other-Services stack in Media Server at Proxmox PC On-Prem The request will only reach Traefik at OCI-Bom-Arm. Traefik will check it’s backend i.e, File and Redis. If service is found, it will redirect to Keycloak and authenticated via: Native Auth if : service supports OAuth and OAuth is setup at service Middleware Auth if: no native OAuth is setup AND oauth middleware is applied at service But how does Traefik know where Dash service is at? This is where Redis backend and Tailscale come to the picture! Traefik-Kop is a service which writes to Redis over at OCI-Bom-ARM and traffic is routed using Headscale and Outbound NAT at OPNsense Thanks! Thank you for going through this long, complicated story of my Homelab, do leave your comment, react and share this with your Tech Ninja Friends!" } ]
