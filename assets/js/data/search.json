[ { "title": "How a Browser Works: From Text to Pixels (1/4)", "url": "/posts/web-fundamentals-part-1/", "categories": "chaicode, web-fundamentals-series", "tags": "networks, browsers, web-fundamentals, chaicode", "date": "2026-01-30 04:20:00 +0530", "content": "Ever wondered how a browser takes HTML from the internet and turns it into pixels on your screen? And how we get beautiful websites… without writing low-level rendering engines in C? Browser Internals When you “open a site” the browser gets at work. To make this possible, browsers are built from multiple subsystems working together. Here are the core components: User Interface: The UI is what the user interacts with directly. This is everything outside the webpage itself. It includes: Address bar Back/forward buttons Tabs Bookmarks DevTools Rendering Engine This is the part which turn content (HTML and CSS) into shiny pixels. Some common rendering engines include Blink for Chrome, Gecko for Firefox and WebKit for Safari Rendering Engine is responsible for: Parsing HTML/CSS Constructing DOM + CSSOM Running layout Painting pixels Browser Engine The browser engine is the coordinator which sits between the UI layer and the rendering engine Browser Engine is responsible for: Managing page navigation Handling reloads and history Mapping user actions on the to rendering commands Networking Layer Networking Layer handles all internet communication, like: DNS resolution TCP connections TLS encryption HTTP requests/responses JavaScript Interpreter / Engine JS Engine interprets and executes JavaScript code. Some of the common JS Engines are V8 for Chrome, SpiderMonkey for Firefox and JavaScriptCore (JSC) for Safari JS Engine runs scrips, handles events and modify DOM dynamically UI Backend UI Backend takes input from the UI and connects the browser to the system API It utilizes the said system API for fonts, window drawing, input events and much more Disk API Disk APIs provides session persistence, fast reloads and Offline support Browsers persist data through storage cache, cookies, LocalStorage etc Visualization Here is a diagram to visualize a few components of the browser! Browser Components Parsers Once the browser has HTML and CSS, it needs to turn the text into something which could be made sense of for rendering. This is where parsing comes in the picture. Parsing MDN defines parsing as such Parsing means analyzing and converting a program into an internal format that a runtime environment can actually run Let’s take a look at a mathematical equation to actually understand what this definition even means. The equation: 2 + 3 × 4 would be parsed to something like this: + / \\ 2 × / \\ 3 4 HTML Parsing Similar to the example, when browser receives HTML, the HTML parser performs a similar operation. For example: &lt;h1&gt;Welcome&lt;/h1&gt; &lt;p&gt;to Hudater's Blog&lt;/p&gt; would be parsed as: Document └── body ├── h1 │ └── \"Welcome\" └── p └── \"to Hudater's Blog\" This parsed tree is sent to the Content Sink. Content Sink takes parsed tokens, create the corresponding DOM objects and insert them into the document tree CSS Parsing And the same for CSS. h1 { color: blue; } p { font-size: 16px; } would be parsed as: Stylesheet ├── h1 -&gt; { color: blue } └── p -&gt; { font-size: 16px } DOM and CSSOM The tree parsed from the previous step is then turned into object models for both HTML and CSS DOM DOM stands for Document Object Model It is the Object Model made from parsed HTML Defines the structure of the page CSSOM CSSOM stands for CSS Object Model It is the Object Model made from parsed CSS Defines how the structured page would be stylized with styling rules CSSOM creation blocks rendering Rendering Now that we have the Object Models, we can look into the final process of rendering. But as usual with browsers, it is a multi-step process consuming several components. Reflow Reflow is also knows as Layout. At this stage, the browser computes geometry. We now have a layout of how the page would be rendered. Painting Once the browser has a layout, it needs to draw the elements. The process of painting converts elements into actual visual instructions like: Draw background Draw border Draw text Draw images Draw shadows At this stage, we have: Layout List of drawing operations Compositing Compositing is also knows as Display. Modern browsers break the page into multiple layers such as background layer, text layer etc The browser uses the GPU compositor to combine these layers efficiently At last, we have pixels rendered on our display Visualization Here is a diagram to visualize how plain text turns into Pixels! Rendering Process Conclusion So we now understand browsers! Well, definitely not in the first reading off this article. If this is your first few times getting to read about browser internals, I would highly encourage you to read further into it. Now that we have this general overview of our browser, it doesn’t seem magical!" }, { "title": "TCP Explained: 3-Way Handshake (6/6)", "url": "/posts/networking-part-6/", "categories": "chaicode, networking-series", "tags": "networks, protocol, chaicode", "date": "2026-01-29 20:00:00 +0530", "content": "Ever wondered why your browser doesn’t just start sending data? How TCP makes the connection? Recap From the previous blog, we can recall that: TCP is a reliable protocol TCP has a certain process to establish connection TCP is stateful since both client and server maintain connection state TCP provides ordered delivery of data using sequence numbers Primer TCP starts transmitting data upon establishing the connection. In this blog, we’ll see how TCP establishes a connection, transfers data reliably, and terminates the session cleanly. Sequence Number TCP splits application data into segments based on the MSS of network connection which itself is derived from MTU This requires larger data to be broken into parts thus TCP uses Sequence Numbers A sequence number is a number TCP assigns to every byte of data sent in a connection. It acts just like page numbers in a book. It tells the receiver exactly where each piece of the transmitted data belongs. Both sides agree on starting sequence numbers before exchanging real data. Sequence number is the reason TCP can guarantee Ordering and detect missing data. Connection The TCP 3-Way Handshake is the process TCP uses to establish a connection between two devices before any real data is exchanged. TCP cannot just start sending packets immediately, because it must ensure: The other side is reachable Both sides are ready to communicate Sequence numbers are synchronized The connection state is properly initialized This connection setup happens in three steps, hence the name: SYN SYN-ACK ACK Only after this handshake does TCP begin sending application data. Handshake Let’s assume your computer is the client and a website server is the server The process of handshake and acknowledgments make TCP reliable Client -&gt; Server: SYN The client starts the connection by sending a packet with the SYN flag set. SYN stands for “Synchronize” At this point, the client is just sending a message to the server saying: “I would like to start a connection, Here is a sequence number” Packet situation; Client -&gt; Server: SYN (seq = n) Server -&gt; Client: SYN-ACK The server receives the SYN, and responds with two things: SYN: A request to Synchronize with the client ACK: Acknowledgment for the SYN Packet of client Server also sends over it’s own sequence number AND adds 1 to the client’s sequence number Packet situation; Server -&gt; Client: SYN-ACK (seq = m, ack = n+1) Client -&gt; Server: ACK Finally, the client sends back an ACK packet confirming the server’s response. Packet situation; Client -&gt; Server: ACK (ack = m+1) Visualization Here is an image visualizing the 3-way Handshake Process 3-Way Handshake At this point, a bi-directional connection has been established between the client and the server. Data transmission can now begin Checksums TCP includes a checksum in every segment. This ensures the data was not corrupted in transit. If corruption occurs during transmission: The segment is discarded Receiver doesn’t send back ACK Sender retransmits the packet Checksums ensure data integrity Retransmission The sender assumes packet loss when: It does not receive an ACK for a segment within the expected time It receives multiple duplicate ACKs, indicating the receiver is still waiting for the same missing sequence range The retransmission timeout (RTO) is reached Once packet loss is detected, TCP retransmits the missing segment to ensure reliable delivery. Retransmission ensures data is sent and received as whole Retransmission Timeout (RTO) is a mechanism that triggers when a sender does not receive an ACK for transmitted data within a specific timeframe Connection Termination Just like TCP requires a handshake to start, it also requires a proper process to close. TCP terminates connections using a 4-step FIN handshake. Why 4 way? Why not just 2 packets? TCP closes each direction independently, so both sides must finish sending Client Sends FIN The client, when it wants to close the connection, sends a packet with the FIN flag set. FIN stands for “Finish” At this point, the client is just sending a message to the server saying: “I have no more data to send, I would like to close this connection” Packet situation; Client -&gt; Server: FIN Server Sends ACK Server receives the FIN packet and acknowledges the Finish request Packet situation; Server -&gt; Client: ACK Server Sends FIN Server tells the Client that it too doesn’t have any more data to send and would like to close the connection Packet situation; Server -&gt; Client: FIN Client Sends ACK Client acknowledges and sends the last ACk packet Packet situation; Client -&gt; Server: ACK At this point, the TCP connection has been gracefully closed! Conclusion That was it in this series of blogs for networking. At this stage, we have a solid understanding of how data is transmitted over the web. In further blogs, we will delve inside the Browser and polish up some Web fundamentals. FIN!" }, { "title": "TCP vs UDP Explained: Reliable vs Fast Networking (5/6)", "url": "/posts/networking-part-5/", "categories": "chaicode, networking-series", "tags": "networks, protocol, chaicode", "date": "2026-01-29 17:00:00 +0530", "content": "What if your video calls ran 30 seconds behind? What if your file download missed random chunks? This is where the speed vs reliability debate comes into place Data Transmission Looking at how us humans communicate, we can deduce few simple facts. The most basic one being: We need common language to communicate with each other. But there are certain rules and etiquette to this communication. Most people follow them which creates a general flow for conversations: We start with greetings We take turns speaking We try not to interrupt And we end with a goodbye Similar to this, the wizards of Internet created a set of rules which computers need to follow to ensure communication over Networks. Protocols These set of rules are called Protocols. A protocol defines things like: How data should be sent How the receiver should respond What happens if something is lost And how communication begins and ends Without protocols, computers would not have rules and etiquettes. A communication without rules turns into just a mess of words. Two of the most important protocols in networking are TCP and UDP. Both of them live at the transport layer in the OSI Model.Both of them sit on top of IP, which handles addressing and routing We can summarize that: IP handles routing packets to the correct machine TCP/UDP decide the rules for how the data is actually transmitted once it gets there TCP TCP stands for Transmission Control Protocol. TCP is the protocol used when accuracy matters more than speed. It behaves like a proper conversation: It establishes a connection first It keeps track of what has been sent It expects acknowledgements from the receiver And it retransmits anything that gets lost Here is a diagram showing TCP Flow TCP Flow UDP UDP stands for User Datagram Protocol. It takes the opposite approach and is designed for speed and simplicity. It sends data with minimal overhead, meaning there is: No connection setup No tracking No guarantees No retransmissions Each packet is sent independently, almost like shouting a message and moving on. UDP is stateless, each packet is independent Here is a diagram showing UDP Flow UDP Flow Use Cases UDP A common beginner mistake is to think UDP is useless since there are no guarantees attached with it. After all, why would anyone choose a protocol that doesn’t promise delivery, ordering, or retransmission? But that’s the point of UDP! It is fast because there is no handshake and there are use cases for it. For example: Video and voice calls Online gaming Live streaming DNS queries For all these use cases, a few dropped packets are fine but buffering or delayed response is not. TCP Being the conservative protocol TCP is, there are multiple obvious use cases for TCP: Web browsing File transfers (FTP, SFTP, Downloads) Email (SMTP, IMAP, POP3) Remote access (SSH) For all these use cases, intact data delivery is integral. You don’t want your Linux ISOs missing a few Megabytes! From the information we gained till now, it’s pretty clear there are pros and cons to both. We should now clear out some misconceptions about protocol layering. HTTP and TCP So we know TCP works on top of IP but what about HTTP? First things first, HTTP != TCP TCP is a transport protocol. Its job is to make sure data can travel reliably between two computers HTTP is an application protocol. It defines the rules of communication, such as what’s a request, what’s a response and so on Final Layering So the stack looks like this: Layer Protocol Role Application HTTP Web requests and responses Transport TCP Reliable delivery Network IP Routing packets across internet They come together and form a beautiful request-response cycle! Conclusion So now we understand why different protocols exist, let’s start diving into TCP in the next blog!" }, { "title": "cURL: The Internet, Stripped of Its UI (4/6)", "url": "/posts/networking-part-4/", "categories": "chaicode, networking-series", "tags": "networks, dns, chaicode", "date": "2026-01-29 06:30:00 +0530", "content": "You click Email at hudater.dev to offer me a Job Mail client opens But what if I told you that button is just a very polite wrapper around a command-line tool? Well it’s not exactly cURL under the hood but the principle remains the same: a client sends a request, and a server sends back a response. What you triggered with that button click is exactly what cURL does but the good, ol’ CLI way: a request and a response This simple, UNIX-like interaction of request-response is what makes cURL all that useful for us. Terms We should clarify some terms before starting with cURL. Client A client is anything that makes a request. Examples GUI Browser like Chrome cURL If something is asking for data, it’s the client. Server A server is the machine (or program) that responds to requests. Examples: A website backend An API service Request A request is the message sent from client to a server. It usually includes: What you want Where you want it from Sometimes extra info (headers, authentication) Example request: “GET /users” Response A response is what the server sends back upon a request It contains: A status code (success or failure) Data (HTML, JSON, etc.) Example response: {“username”: “Hudater”} API (Application Programming Interface) An API is basically a server endpoint meant for programs, not humans. Instead of giving you a webpage, it gives you data. Data can be in format like: JSON XML Plain text Meant for programs to consume, not humans Protocol A protocol is a set of rules for communication. HTTP (HyperText Transfer Protocol) HTTP is the basic communication language of the web. It defines rules for how requests and responses work: HTTPS HTTPS is just HTTP, but encrypted and secure. The “S” stands for Secure URL (Uniform Resource Locator) A URL is the address of something on the internet. Example: https://hudater.dev Status Codes When a server responds, it includes a number showing what happened Some common Status Codes: 200 - OK (success) 404 - Not Found 500 - Server error cURL Explainer At its core, cURL is a tool to transfer data using URLs. cURL supports multiple protocols but one we will use here is HTTP and HTTPS. It’s like the networking part of a browser minus the GUI. Why cURL We need cURL because it gives the most direct way to communicate with servers. It shows the server’s response as-is, including the raw output and important details like HTTP status codes. This makes cURL extremely useful for testing and debugging applications without relying on a browser or frontend. cURL is especially common when working with APIs, where the goal is to send requests and receive data directly, usually in formats like JSON. cURLing away Let’s try cURLing my links webpage curl https://hudater.dev cURL Webpage This command sends a simple HTTP GET request to the server and prints the response directly in your terminal. In the response screenshot, we can see we get pure HTML back which would be rendered if we sent the request via a GUI Web Browser Verbose That wasn’t very useful for debugging. Let’s try the -v flag which will enable verbose output mode showing us more information about the connection, request and response curl -v https://hudater.dev cURL verbose Header cURL verbose Footer If you executed this cURL command, you would be able to see: DNS Lookup TCP Connection (handshake) HTTP Request being made Response Status Code Response Headers Specifying HTTP Verbs The default behavior when running cURL is to perform a GET operation but we can specify operation verb using: curl --request HTTP_VERB_HERE URL_HERE or curl -X HTTP_VERB_HERE URL_HERE GET Let’s do a GET operation on an API from FreeApi.app curl -X GET https://api.freeapi.app/api/v1/public/randomjokes/joke/random cURL GET Response POST without Parameters The POST method is used to send data to the server. It is a simple write operation. Once the POST request has been made, the server can process the data. Let’s do a POST operation on the Kitchen Sink API Endpoint from FreeApi.app curl -X POST https://api.freeapi.app/api/v1/kitchen-sink/http-methods/post cURL POST Response Since this is just a Kitchen Sink Endpoint, meaning it’s just there to practice and sends back 200 response code upon receiving POST request POST with Parameters Let’s do a POST request to Register user API endpoint, again from FreeApi.app curl -X POST https://api.freeapi.app/api/v1/users/register cURL POST Fail In the response, we get Status Code 422 which means the request has Unprocessable Content. From the API documentation, we can see that we need to provide certain data with the request. Now we will do a POST request with correct data curl --request POST \\ --url https://api.freeapi.app/api/v1/users/register \\ --header 'accept: application/json' \\ --header 'content-type: application/json' \\ --data '{ \"email\": \"user.email@domain.com\", \"password\": \"test@123\", \"role\": \"ADMIN\", \"username\": \"doejohn\" }' cURL POST Success This time we get Status Code 2XX which means the request was successful! This command has some extra flags, here is an explanation: Flag Meaning –header ‘accept: application/json’ Tells the server to send back data in JSON Format –header ‘content-type: application/json’ Tells the server you are sending data in JSON Format –data ‘{“key” : “value”}’ This is the data you are sending Common Pitfalls Beginners are bound to make mistakes, most we can do is beware! Here are some common mistakes to avoid: Mixing up lowercase and uppercase Flags For example: -X is equivalent to --request while -x is used to specify proxy to use Sending data without specifying data format This could cause Failed method call, make sure to specify your sent data format Not Using Quotes Around Data Improper quotes would cause data to be interpreted as shell commands, watch out! Visualization Here is a diagram explaining Browser flow Browser Interaction And another one explaining cURL flow cURL Interaction Conclusion Now we understand what cURL is, how to make requests using cURL and much more. In the next blog, we might be doing some handshakes ;)" }, { "title": "How DNS Actually Works: Root Servers to Your Browser (3/6)", "url": "/posts/networking-part-3/", "categories": "chaicode, networking-series", "tags": "networks, dns, chaicode", "date": "2026-01-29 03:00:00 +0530", "content": "How exactly did hudater.dev packet travel? Did it fly its way over that Github Server? Well, it worked it’s way through a chain of authority. Let’s dive in and travel with our packet! Quick recap DNS (Domain Name System) is the system that converts human-friendly names into machine-usable IP addresses. Hooman wants: hudater.dev Computers know: 185.199.111.153 All DNS does is create a bridge between these two parties so when you type hudater.dev you get a website and don’t need to remember a string on dot-separated numbers DIG In this blog, we will go through this global internet dictionary called DNS using our beloved tool: dig From Arch Man pages: dig is a flexible tool for interrogating DNS name servers. It performs DNS lookups and displays the answers that are returned from the name server(s) that were queried. Most DNS administrators use dig to troubleshoot DNS problems because of its flexibility, ease of use, and clarity of output. Other lookup tools tend to have less functionality than dig. Bare dig and NS So dig performs DNS resolution for you, great! Let’s try and run it. dig dig output When you run dig without any options, it will default to running dig . NS in most cases but this is dependent upon the package maintainer From the last blog, we know that Name Servers (NS) are used to define who the authority for a domain name is. Since we effectively executed dig . NS, we get the Name Servers for root zone i.e, the root of DNS The response we get in ANSWER SECTION is formatted this way: ZONE TTL CLASS TYPE VALUE . 81275 IN NS f.root-servers.net. . 81275 IN NS g.root-servers.net. . 81275 IN NS h.root-servers.net. . 81275 IN NS i.root-servers.net. . 81275 IN NS j.root-servers.net. . 81275 IN NS k.root-servers.net. . 81275 IN NS l.root-servers.net. . 81275 IN NS m.root-servers.net. . 81275 IN NS a.root-servers.net. . 81275 IN NS b.root-servers.net. . 81275 IN NS c.root-servers.net. . 81275 IN NS d.root-servers.net. . 81275 IN NS e.root-servers.net. Explanation of ANSWER SECTION in the response Value Meaning . The Root Zone 81275 TTL = cache for 81275 seconds (this can change) IN Internet class NS Nameserver record h.root-servers.net One of the root DNS servers (a-m: 13 logical root servers) Root Servers You might have heard there are 13 servers responsible for the whole internet! or something along those lines. Well that’s an overstatement. The fact is: There are 13 logical server identities Each identity is backed by thousands of servers Here is the URI to official root servers website Root servers are are authoritative for the Root Zone (.) These root servers store a few thing, most important one of those for us: Which nameservers are authoritative for each TLD zone. Resolver and dig Empty NS Response Now we can use the dig command to further drill down into specific TLDs. Let’s check the dev TLD dig dev. NS dig dev NS failed output Looking at that response, we didn’t get any Name Servers and the status NOERROR We just got ourselves an empty NS response due to our DNS Resolver. Resolver Delegation Metadata From our previous dig response, we can see the resolver which was used SERVER: 100.100.100.100#53(100.100.100.100) (UDP) By default, dig will use whatever is your system’s current DNS Resolver which in my case is 100.100.100.100 #53 represents the port used which is 53 for DNS by default UDP is the protocol which was used, again, the standard default for DNS In a Linux system, your DNS Resolver is specified in the /etc/resolv.conf file. For me, it’s my headscale’s DNS resolver which is Pi-hole forwarding queries to Unbound, which acts as a recursive resolver for my tailnet. From what I understood with my limited knowledge and research, this chaining caused resolvers to optimize for answering domain lookups thus we were not able to get delegation metadata for TLD zones. I would be doing further research into this odd behavior, if you understand why did this occur, please reach out to me via mail or comment on this blog Long story short, We need to query the root servers from our first dig response. We can specify any DNS Resolver to dig something like this: dig hudater.dev NS @1.1.1.1 TLD Authority Querying a root authoritative server directly, we can execute: dig dev. NS @a.root-servers.net. dig dev NS correct output The response we get in AUTHORITY SECTION is formatted this way: ZONE TTL CLASS TYPE VALUE dev. 172800 IN NS ns-tld1.charlestonroadregistry.com. dev. 172800 IN NS ns-tld3.charlestonroadregistry.com. dev. 172800 IN NS ns-tld5.charlestonroadregistry.com. dev. 172800 IN NS ns-tld2.charlestonroadregistry.com. dev. 172800 IN NS ns-tld4.charlestonroadregistry.com. Explanation of AUTHORITY SECTION in the response Value Meaning dev. TLD dev Zone 172800 TTL (this large since NS rarely changes) IN Internet class NS Nameserver record ns-tld1.charlestonroadregistry.com. One of the authoritative DNS NS for TLD dev (registry owned by Google) As we can see from the response we get, our authority has changed. So now we need to use that authority to go further down into chain to query for a FQDN under the dev TLD Domain Authority Let’s check for hudater.dev dig hudater.dev. NS @ns-tld1.charlestonroadregistry.com. dig hudater.dev NS output Again in the above response we get 2 Authoritative NS for the domain hudater.dev This time the NS are by Cloudflare, which is correct since my domain hudater.dev is using Cloudflare as its Name Servers Final Authority We will now go further into this chain and query the Cloudflare NS from this response dig hudater.dev. NS @jaziel.ns.cloudflare.com. dig hudater.dev NS AA output This is the final Authoritative Answer for our domain! But how do we know for sure this is the final authority? The highlighted aa flag in the response Header stands for Authoritative Answer This denotes that Cloudflare is authoritative for the zone that owns hudater.dev To further verify this claim, we can enquire for Start of Authority (SOA) Record dig hudater.dev SOA @jaziel.ns.cloudflare.com dig hudater.dev SOA output Since we get an answer from this NS for SOA record, we can be sure this is the Authority for this zone. If we used any other NS in this command, we would not get a SOA answer Domain IP from Authority To get the IP for this domain, we now need to query the Authoritative Cloudflare Name Servers from the response for A records dig hudater.dev. A @jaziel.ns.cloudflare.com. dig hudater.dev A output Finally, we have the IP! And if you recall from the previous blog, our A records are correct. Conclusion Here is an image representing all this flow for DNS in a visual way Hudater.dev DNS Flow Good for us there already exists DNS resolvers which would do all this just to feed us with webpages!" }, { "title": "DNS Record Types: Names and Pointers (2/6)", "url": "/posts/networking-part-2/", "categories": "chaicode, networking-series", "tags": "networks, dns, chaicode", "date": "2026-01-28 19:30:00 +0530", "content": "You type hudater.dev On your screen is a beautiful links page But pause for a second. Did my computer just guess the website location? Did the page arrive through magic? Of course not. This is DNS at work. IP Address It all started with some numbers and dots. Remember the network we created previously? When you connect to it via Ethernet or WiFi, you are assigned an IP Address IP stands for Internet Protocol and the service which assigns you an IP automatically is called DHCP. An IP assigned by this DHCP server is known as an DHCP Lease since it’s not static, by default, in most cases IPv4 IP Address is 32 bit number of 4 octets separated by dots. 4 octets mean 8 bits each. What this jargon translates to is this: We can have max 2^32 unique addresses (~4.3 billion) Something like: 192.168.1.1 But never 300.300.320320.3232 because that’s not 8 bits for each octet IPv6 When networks were first created, these 4.3 billions seemed like a lot. What the creators underestimated was the drive of Human Laziness A few smart toilets here, smart bulbs and now we have run out of IPv4 addresses. Please welcome IPv6 Instead of 32 bits, we now have 128 bits total, approx 3.4 * 10^38 addresses. These addresses look something like this 2001:db8::8a2e:370:7334 IPv5 Why did we skip IPv5? Well, there used to be something like IPv5 in early 90s. It didn’t exist as IPv5 but rather Internet Stream Protocol It was never deployed publicly but was an experimental protocol to implement real time communications like video calls IP and Servers When you deploy an EC2 instance on AWS or connect your router to your ISP, you are assigned a Public IP Address. Public Address because others on this wide open internet need to communicate with you and vice versa so all this convoluted setup serves a purpose If we allow it in our firewall, anyone can hit our IP Address and would be served with content we set-up like a website! Remember these bits So now we have an address for our computer on the internet with a website on it. Everyone just needs to remember the IP Address. Yea, that doesn’t seem like a good idea. Flat Names We should give it a friendly name like mybeautifulsite If you floated that idea to a Senior Dev, you would get a smack on your head and an explanation something like this: Simple strings as names mean no hierarchy No hierarchy translates to a huge single list of names There is a limit to a string being both memorable and typeable for humans It would not scale Hierarchal Names So we need to add hierarchy to names. Taking inspiration from Filesystems, we use dots(.) Root Root of this whole hierarchical naming system is denoted by a single dot . It’s just there for representation, you it doesn’t have any IP address associated with it. TLD One level below that are TLDs (Top Level Domains) The dev in hudater.dev is the TLD. What TLD provides is a clear separation of concern. Second-Level Domain One level below TLD exists Second-Level domain. The hudater in hudater.dev is the Second-level domain This form of domain name is probably the most common. This is the address we can buy for ourselves and use for our website. Also known as Registered Domain Subdomain From this point on, we can create Sub-domains and sub-subdomains and so on blog in blog.hudater.dev is the subdomain Visualization . (root) └── dev. (TLD) └── hudater.dev. (domain you own) └── blog. (subdomain) └── gh. (sub-subdomain) Fun Fact: You can add an extra dot at the end of any domain name and it behaves the same. That final dot simply represents the DNS root (.), because every FQDN technically ends at the root. hudater.dev == hudater.dev. FQDN This whole name, i.e, gh.blog.hudater.dev is knows as Fully Qualified Domain Name or simply Domain Name DNS Records Nameservers You own a domain, but how would Browser know where to ask for your domain’s addresses? This is where Name Server Records come into picture. NS Record tells who the authority for a certain domain is Typically, the place where you bought your domain from is the Authority for Domain but it can very easily be changed to any other provider, Cloudflare being one of the famous providers A Record A record, also knows as Address Record, points a FQDN to an IPv4 address. It is the most basic DNS Record which you would most probably use for your website For example: Record Type Name Content A hudater.dev 185.199.108.153 A hudater.dev 185.199.109.153 A hudater.dev 185.199.110.153 A hudater.dev 185.199.111.153 But there are multiple A records for same target, which IP would be returned to client? DNS resolver would typically utilize a load balancing technique such as Round Robin to return an IP to the client. AAAA Record Just like A record, Quad A records point to an IP but this time it is IPv6 Why four A instead of single A? IPv4 is 32 bits while IPv6 is 128 bits. 32*4=128 therefore 4 As CNAME Record CNAME stands for Canonical Name. A CNAME record points to an A Record or another CNAME Record CNAME Records are useful when you need to point multiple domain names or FQDNs to a single IP For example: Record Type Name Content CNAME blog hudater.dev CNAME docs hudater.dev CNAME Record is commonly used to point to a domain where you don’t control the DNS records such as Github Pages, Hashnode etc MX Record Mail eXchange Record tells the sender where email should be delivered. Typically, each domain has multiple MX Records which point to different mail servers for redundancy Each MX Record has a priority value. Lower priority value is preferred For example: Record Type Name Content MX hudater.dev route1.mx.cloudflare.net MX hudater.dev route2.mx.cloudflare.net MX hudater.dev route3.mx.cloudflare.net TXT Record TXT records are just plain, old text. TXT Records are used to verify ownership of domain, email security etc Let’s Encrypt uses TXT record to verify domain ownership when requesting a TLS Certificate using DNS Challenge (don’t look up how we used to get TLS Certs before Let’s Encrypt) To protect against mail spoofing attacks, SPF, DMARC and DKIM protocols are used which are based on DNS and executed via TXT records Read more about Mail security here! TTL Time To Live (TTL) is a numerical value (in seconds) associated with a DNS record. To ensure fast and efficient responses, DNS answers are cached at multiple layers, including the browser, the operating system, and most importantly recursive DNS resolvers. A cached DNS entry remains valid only for the duration of its TTL. Once the TTL expires, the cached copy is discarded. The next time a query is made, the resolver must perform a fresh lookup by contacting the authoritative name servers (following the DNS hierarchy if needed) until it retrieves the current IP address. Real World Scenario Table Let’s take a look at a real world setup where a domain has A,CNAME, MX and TXT records setup: Record Type Name Content A hudater.dev 185.199.108.153 A hudater.dev 185.199.109.153 A hudater.dev 185.199.110.153 A hudater.dev 185.199.111.153 CNAME blog hudater.dev CNAME docs hudater.dev MX hudater.dev route1.mx.cloudflare.net MX hudater.dev route2.mx.cloudflare.net MX hudater.dev route3.mx.cloudflare.net TXT _dmarc “v=DMARC1;p=quarantine;rua=mailto:harshit@hudater.dev;ruf=mailto:harshit@hudater.dev;” TXT cf2024-1._domainkey “v=DKIM1; h=sha256; k=rsa; p=a_random_public_key” TXT hudater.dev “v=spf1 include:_spf.mx.cloudflare.net ~all” Visualization Here is a flowchart style diagram visualizing this setup into pixels Hudater.dev DNS Setup Explanation This is my own domain’s DNS setup. My registered domain (hudater.dev) has multiple A records which point to Github pages IP. As we know now, this will make sure traffic to hudater.dev would be load balanced and not dependent upon a single server (most probably, single Load balancer in front of a cluster of servers) Since blogs.hudater.dev and docs.hudater.dev are both hosted on Github pages, they are CNAMEs pointed to my A records. Incoming email at harshit@hudater.dev is handled by Cloudflare email routing and outgoing email is handled by smtp2go which are both facilitated using MX and TXT records alongside Cloudflare email routing rules. All this DNS shenanigans for all my domains are managed using OpenTofu, HCP Cloud and Github actions. Take a look at that here!" }, { "title": "Networking Fundamentals: Understanding Network Devices (1/6)", "url": "/posts/networking-part-1/", "categories": "chaicode, networking-series", "tags": "networks, devices, chaicode", "date": "2026-01-26 04:50:00 +0530", "content": "You’ve been lied to! Your router is not just a router There are hidden miniature alien being inside it Let me explain You might have used the internet (or are you a mysterious alien from Andromeda who has gained the ability to access this webpage through some magic!) But you might not understand how it works. How can a company come and install one weird looking box with blinking lights and antennas and suddenly you can talk to your uncle sitting in another continent! In this series of blogs, I’ll take you on a journey through air to wires and at the end, you might just understand Why Internet is Magical and How sometimes a Shark might just try to upload some fresh data in the undersea cable network The WiFi Box Enigma The device your ISP installed does not serve a singular function, it actually entails a number of devices cramped into one functional unit. Let’s break open this ISP-provided box and see what’s really inside. Modem and ONT Modem Modem stands for Modulator/Demodulator. It converts analog signals from your ISP into digital signals and vice-versa.This ensures that both ends send and receive data they understand. BUT most likely, in this modern world, you are using Fiber Internet. Fiber doesn’t use a modem rather an ONT. ONT ONT stands for Optical Network Terminal. It works similar to Modem, converting Light Media from Fiber Optic to Electrical Signals which your Ethernet cable carries Conclusion: Modem and ONT are just bridge devices which convert media formats between two mediums. Hub and Switch Whether you are using Modem or ONT, you now have an Ethernet connection at this point. You very well can plug this Ethernet in your PC and get an IP from your ISP using their Authentication Method But this method is not secure, we will get to it once we talk about Firewalls Another concern with this type of setup would be expandability. You would have internet connection to just a single PC, no WiFi! To overcome this concern, we need to plug this single connection into such a device which would take a single Input and give us Multiple Outputs. Thankfully for us, God gave us Hubs and Switches! Hub Hubs are just multiport repeaters. They take traffic from the Uplink(or Input) and forward it to every other port on the hub. They operate at Layer 1 of OSI Model which is the lowest layer meaning they have no understanding of MAC addresses, IP addresses, or frames. They simply deal with raw electrical or optical signals on the wire. The limitations with the Hub approach are quite obvious. Since Hubs effectively forward all traffic to all ports it results in collisions, congestion and slow speeds. To mitigate these limitations, Switch came into picture. Switch Switches operate at OSI Layer 2 (the Data Link layer) and serve a similar role to hubs: they connect multiple devices within the same local network. However, unlike hubs, switches are intelligent forwarding devices. A switch learns the MAC addresses of connected devices and sends frames only to the port where the destination device resides. This prevents unnecessary traffic flooding, reduces collisions, and makes modern Ethernet networks far more efficient. Internally, switches operate using a CAM table CAM table basically tells which MAC address is reachable through which physical port How does a Switch learn all that? From the CCNA study material. Just kidding, it is a multi-stage process but before learning this process, we need to clarify what a frame is. At this stage for us, we can think of frames to an equivalent of packets on layer 2 whereas a packet is a layer 3 construct. Here is a good explanation by a kind reddit user. Now onto the CAM table creation process: MAC Learning A frame enters Port n on the switch Switch takes a note of the Source MAC Address and associates it with the Port n Forwarding Switch checks the Destination MAC Address on the frame Unknown Unicast Flooding If the switch doesn’t have an entry for the Destination MAC Address, it floods the frame out all ports (except the incoming one) Once the correct destination receives the frame, it replies an acknowledgement to the Switch This Ack is then recorded as the MAC Address for the destination port Unicast Forwarding If the switch already has an entry for the Destination MAC Address, it forwards the frame to the port known in the CAM table Table Aging Since destination devices might be changed after sometimes, this CAM table is not permanent Every entry has an associated Last Seen time Once a device hasn’t been contacted till a certain time (typically 300 seconds), it’s entry is deleted from the CAM table Role in a modern consumer router Why did we learn all this about Switch and Hub? Because your Home Router comes with one! The Ethernet Ports on your router are just in-built switch in that router. You can also install a separate ONT+Switch combo to make your own router+firewall combo just like I do at my homelab! More about that later. Router Router is what routes the traffic! [insert Mind Blown GIF here] Okay, let’s get a bit more technical. A router is a Layer 3 device that forwards IP packets between networks based on: Destination IP address Routing table lookup Next-hop resolution Encapsulation rewrite For begineers, a router can be thought of as a logical program which takes decisions for the packets (packets since we are at Layer 3 now) path hop-by-hop Firewall A firewall is the security checkpoint of a network. Its job is to decide what traffic is allowed to enter or leave your network, and what should be blocked. Typically in a home network, firewall doesn’t allow any incoming traffic but allows all outgoing traffic. Typical Inward Flow If you want to, let’s say, run a web server on your PC and want it accessible to the web, you’ll need to configure your firewall to: Allow incoming traffic at certain port (80 and 443 for HTTP and HTTPS by standard) Specify which protocol would be allowed (most likely TCP) Specify MAC Address of Destination PC Doing this is not recommended unless you know what you are doing. Why Security lives here? Firewall ensures the security of a network by checking every packet against the set of rules specified to it. If a packet fails, it’s dropped. Otherwise, it’s forwarded to the router. This filtering of traffic based on a rules-outcome scenario is why security lives at the Firewall. Access Points The antennas on your home router are actually part of the wireless access point component inside the device. An access point is responsible for providing Wi-Fi. It simply acts as a bridge between wireless clients (phones, laptops) and your wired local network. Load Balancer Load Balancer (often abbreviated to LB) is a device or service that sits in front of multiple servers and distributes incoming traffic across them. This ensures an optimal flow of traffic to the whole cluster of servers WHILE STILL being seen as a single unit to the outer Internet LBs operate at both Layer 4 and Layer 7. Traefik is a popular Load Balancer which works primarily at Layer 7 but also has limited Layer 4 capabilities. This is the LB I use in my homelab! To read more about L4 vs L7 LBs, read this blog by HAProxy Real World Setup So now that we know of these devices, let’s try and break open your router! Let me visualize a router where every component is separately represented Home Router Visualized Conclusion With the knowledge we gained from this blog, we can now build on top and get a better understanding of TCP, UDP and DNS. Maybe a bit of CURL action too, who knows!" }, { "title": "Inside Git: Objects, References, and the Myth of Magic (3/3)", "url": "/posts/git-part-3/", "categories": "chaicode, git-series", "tags": "git, vcs, version control, dvcs, chaicode", "date": "2026-01-15 23:50:00 +0530", "content": "“In five days he created Git, and on the sixth day he rested.” Or so they say. The speed makes for a good legend, but what’s worth appreciating is the engineering behind it. The design goal behind Git was straightforward: fast, simple, and precise To understand these design decisions and the quirky behavior of Git, we need to understand the .git directory and not just memorize commands. When we run: git init Git creates a .git directory at the current working directory. The .git directory acts as a database for Git. A little pre-requisite knowledge Before we go any further, we need to clear a few points. These are required understanding before we get into Git Plumbing and Porcelain Git has two types of commands: Porcelain - the friendly commands you use every day (git status, git commit, git log) Plumbing - the low-level machinery that actually makes Git work (git cat-file, git commit-tree) Porcelain commands are built on top of Plumbing commands. Essentially, most Porcelains are just wrappers on multiple Plumbing commands to make Git easier for day-to-day usage We’re going to ignore the Porcelain for a bit and get Plumbing! Git Internals Objects, Trees and Blobs Git is Built on Objects and at its core, Git is a content-addressed object database. Those are just a whole lot of buzzwords to seem smart. In essence: Object database means Git stores data as discrete objects Content-addressed means objects are identified by the content they store, not by names or locations There are four object types, but three of them matter immediately: Blob objects These store file contents. A blob knows only about raw data, nothing about filenames, permissions, or directories Comes from Binary Large Objects Tree objects Trees represent directory structures Ties one or more blobs into a directory structure Can refer to other tree objects, thus creating a directory hierarchy Commit objects Ties aforementioned directory hierarchies together Points to a single root tree Points to one or more parent commits Stores author, committer, timestamps, and the commit message In simpler words Trees: Stores directories Blobs: Stores files Commits: Store when and why the snapshot exists Branch: A movable pointer HEAD: A pointer to a pointer Tags: Pointers with a bit of extra metadata Add and Commit Internals Git Add Running git add internally just prepares data inside the .git directory How Exactly? Reads the content (raw bytes) of the files in your working tree Hashes the file content and stores as a blob object in .git/objects If an identical blob already exists, Git reuses it (this means the file has not changed between commits) Finally, the blob hash is then recorded in the index (staging area) and mapped to the file path Git Add Internal Workflow Git Commit Internally, Git Commit works entirely off the index, not the working tree. Again, how? Git reads the current state of the index, treating it as a complete snapshot. From the index, Git builds tree objects that represent the directory structure. Git creates a commit object that: points to the root tree points to one or more parent commits stores author, committer, timestamps, and the commit message The current branch reference is updated to point to this new commit. HEAD points to the branch, which now points to the new commit. Git commit Internal Workflow Hashes and Integrity You might have noticed that we are constantly using hashes to identify everything in Git But how does Git ensure that the data behind a hash stays true and unmodified? Whenever Git stores an object (a blob, tree, or commit), it: Takes the raw content of the object Adds a small header describing its type and size Computes a cryptographic hash of that data Objects are dependent upon multiple factors which implies their hashes are as well. For example: Blob object depends on the file’s exact contents Tree object depends on filenames file modes blob and tree hashes it points to Commit object depends on the root tree hash parent commit hash(es) author and committer metadata commit message If any one of these dependencies changes, it triggers a domino effect, changing the hash of every object that depends on it. This chain of dependencies is what guarantees data integrity in Git. Conclusion From this series of three blogs on Git, you now have a strong understanding of Why Git exists Basic workflow when working solo Internals of Git At its core, Git is nothing more than hashes and pointers arranged in a clever way." }, { "title": "Git for Beginners: Basics and Essential Commands (2/3)", "url": "/posts/git-part-2/", "categories": "chaicode, git-series", "tags": "git, vcs, version control, dvcs, chaicode", "date": "2026-01-14 00:00:00 +0530", "content": "… The dependency didn’t last. BitKeeper was no longer an option for Linux, previously called Freax. At that point, the project had already outgrown patches, email, and centralized version control. Going back wasn’t possible So Linus Torvalds, the developer behind Linux, did the only reasonable thing left. He wrote a tool that solved the version control problems and called it Git With this context, Git starts to make more sense. Its design centers on scalability rather than immediate intuitiveness. Common Beginner Misconceptions Before touching a single command, we need to clear a few ideas that almost everyone gets wrong when starting with Git. Git is GitHub Git != GitHub It “tracks changes” locally Git can have multiple remotes where you can store your repo GitHub is just a remote managed by Microsoft similar to GitLab and Bitbucket managed by other corps You can host your own remote using software like Forgejo, GitLab CE, Gitea Git automatically tracks my changes You explicitly tell Git to track certain files When you want to “save the current state”, you make a commit A commit is a backup Commits are just like a checkpoint which you create Commits are manually pushed to remote Branches are copies of folders Branches are just a pointer to a commit The commit a branch points to is called the tip of the branch Since branches are just pointers, creating them is cheap and fast This is why people say: “branches are cheap in Git” Git Basics, Workflow and Essential Commands What Git Calls a Repository? A repository (often shortened to repo) is a project tracked by Git. Essentially, it’s just a directory which contains your project files But how does Git identify a git repo? It is identified by a hidden .git directory If a directory has a .git subdirectory inside it, Git considers it a repository. The files you edit live in the working directory. The history and metadata live inside the aforementioned .git subdirectory Most Git commands are simply moving changes between these two spaces. Git Config Git uses configuration files to record who made a change and when OR in more technically correct words: Git uses configuration files to determine the author identity and timestamps recorded in commits. Well, where is this config file? There are a multitude of config files which Git looks for by default. Each config file has a Scope or Level. These levels decide where a setting applies and which setting wins if there is a conflict. Scope Location Applies to System /etc/gitconfig Every User Global ~/.gitconfig Current User Local .git/config Specific Repo Worktree .git/config.worktree Worktree Specific This neat diagram by Boot.dev explains really well which config file overrides which one: Credit: Boot.dev You don’t need to think much about the Git config file at this point of time. Most of the time you are going to use the Global config file Substitute the e-mail and username with your own and run these commands to setup a basic Git config git config --global user.email \"your-email@example.com\" git config --global user.name \"Your Name\" Workflow At a high level, Git workflow looks something like this: We’ll walk through this loop step by step. Working Directory This is where you edit your files, compile, run and test your project BUT at this point it’s just a simple directory. Git has no idea that it even exists. To start using this working directory as a Git repo, we need to initialize it with Git git init This create .git directory in your working directory. At this point Git is NOT tracking any files, it simply exists there ready to be commanded! Check the Current State Make it a habit to run this whenever you start working in a Git repo git status Running git status gives us an overview of what Git sees. A file in a Git repo can have three states: Untracked: Not tracked by Git Staged: File is tracked by Git and will be included in next commit Committed: Saved to repo history Staged files are said to be in “Staging Area” also referred as the “Index” git status also gives you an overview of which files have changed in the repo. Using this information we can move onto the next step Choosing What Belongs Together Good News! You don’t need to record all the changes across files in a single go. We can add the desired files by specifying file names either one-by-one, by-directory, or glob patterns. These files will be added to the staging area git add file.md To add all standard files and directories in current working directory, do git add . Recording a Snapshot Once you’re satisfied with what you’ve selected, you record it. git commit -m \"Describe what changed\" This creates a commit in our Git history. A commit is associated with a long, unique identifier that represents the commit’s contents and metadata called a Hash. Commit hash is derived using various sources such as: content changes commit message Author Name Author e-mail Date and Time Parent commit hash If any of these change, the resulting hash changes as well. Viewing History To perform any operation on or using our history, we need to first be able to see what happened in the past. Git log command allows us to do just that git log This shows you: List of commits Commit messages Authors Order in which commits were made (or reverted) If you just want to see the commit hash and commit message, you can run git log --oneline Conclusion At this stage, you know how to work with git and these are the commands which Developers and DevOps creatures use majority of the time. If you’re stuck with git, try this site In the next post we’ll look inside the .git directory and answer questions like: Where commits actually live What branches really point to How Git stores history on disk Understanding that internal structure is how you become the Git Master!" }, { "title": "Why Version Control Exists: Before Git (1/3)", "url": "/posts/git-part-1/", "categories": "chaicode, git-series", "tags": "git, vcs, version control, dvcs, chaicode", "date": "2026-01-13 23:00:00 +0530", "content": "Subject: Re: build failing on server Hi all, Does anyone still have parser.c from last Thursday’s build? I’m looking for the version prior to the logging changes. The build was stable at that point, and I suspect a regression after that. If someone has it, please send the file. Thanks, a tired developer, 1992 That might have felt like a strange mail, but it was routine to the developers who lived through it This was exactly the kind of friction that forced better tools into existence. To understand why, we need to go back to a time when computers were still strange boxes that took their time “thinking” Mail and Pendrive Era Picture yourself, a young enthusiastic nerd in the 90s, sitting in front of your IBM PC clone running Minix. You and you friends are working on a fun project Freax You know a guy who really knows his way around MASM so you drop a mail asking for help. He is onboard, you send him the code zip over mail and so the collaboration begins. Life goes on, your project is being talked about on IRC Channels and before anyone knows, tens of people are sending over their patches for this revolutionary new software You send latest.zip to Dennis Dennis reverts with an updated parser.c You manually pick and choose changes to keep, compile and run Guido is joining as a new contributor so Tim sends him project.zip You notify Tim about the updated parser, send him the latest_really.zip Here comes Bjarne wholly persuaded by Richard’s take on Libre Licensing and now we suddenly have I_Love_Libre.zip The issue is glaringly obvious. THERE IS NO ORGANIZATION IN THE CODEBASE Everyone has their own version of the truth. This is where early version control systems enter the picture. Early Version Control Systems Centralized VCS There is one central repository that everyone depends on (Source of Truth) You need to be connected to the server in order to make changes Each developer gets a working copy, not the full repository History is on central server CVS There is chatter about version control and there exists this software named CVS. You start using CVS with the help of your friend Ari Now you have one server hosting the Project. Workflow looks something like this: Copy project from Server cvs checkout project Everyone has a local copy on your computer BUT meaningful actions still depend on the server You start modifying main.c on your local FS. There is no lock on files. Someone changed main.c while you were working on it and committed before you? Well, tough luck Update before committing cvs update This the stage where you pull changes from the server (yea change first, pull later) Remember your main.c changes? They are currently in conflict with latest at server. You have to manually pick and choose changes to keep Commit cvs commit After resolving conflicts, you commit your changes. Each file is committed independently - if something fails midway, you can end up with a partially applied change. SVN Someone on the IRC suggested Subversion so you decides to give it a try! The workflow stayed similar Copy project from Server svn checkout project Lock file if you want svn lock main.c -m \"Editing civilization\" You can edit the file locally. Others can still modify it locally, but they cannot commit changes while the lock is held. Commit svn commit main.c -m \"Update civilization\" Now you can commit changes atomically. Some improvement finally! Unlock the File svn unlock main.c The file is now available for someone else Issues with Centralized VCS The server is a Single Point of Failure History is stored on the server making your working copy incomplete Branching while Technically Possible, is expensive Distributed VCS No central server (No source of Truth) Every local copy is a Full Repository including history and branches Branches are Lightweight and Cheap making it part of workflow BitKeeper Freax has become widespread. Hundreds of contributors and Thousands of lines of code CVS and SVN don’t scale to this level so you decided to go Distributed Bitkeeper is a famous DVCS but it is proprietary. Alas! the project needs it so you compromise and start using it The catch is the license. Contributors are explicitly forbidden from reverse-engineering BitKeeper, and violating this condition means losing access entirely. Latest workflow is something like this Clone the Repository bk clone bk://freax.bkbits.net/freax-2.5 freax cd freax You have a full repo copy with all history and branches Make Changes Locally Edit the files you want to change. No need to manually Lock and Unlock files. No connection to the server Commit Changes bk commit Commits were local and Atomic Pull Changes bk pull Pull changes from others. This might create conflict Resolve Conflicts BitKeeper knew which changes conflicted so conflict resolution was easier Merges had explicit Record making it traceable Push Changes bk push Your changes are now available to others. Contributors choose if they want to pull changes or not A new beginning Freax now depends on a proprietary tool, governed by a license that could be revoked at any time. For a project built on open collaboration, that dependency could not last…" }, { "title": "Homelab Part 1 - Hardware, Locations, and Design Goals", "url": "/posts/homelab/", "categories": "homelab, architecture", "tags": "servers, proxmox, homelab, hybrid cloud, architecture diagram", "date": "2026-01-06 05:30:00 +0530", "content": "I have been trying to start blogging and documenting my homelab for a long time (~2 years) but the biggest hurdle has been DIAGRAMS Finally, on this Holy Day of 6th January 2026, I present to the people on the web with my Homelab! This is going to be multi-part blog post which will be roughly divided into 5 parts where I will explain and reason: Why, Goals, Infra Overview Cloud Services Used, Workload Placement Networking, DNS &amp; Traffic Flow Security, Identity &amp; Observability Automation, Ideal Homelab Architecture Why This Homelab Exists I got into Homelabbing and Self-Hosting during 2023 when I was exploring Linux &amp; Docker. Coming across YT Channels like Techno Tim, Chris Titus Tech, IBRACORP, The Lord’s Heir, and his Deepfake made me explore Open Source Tech which eventually led to need of Selfhosting and using Libre Software Design Goals My homelab has been designed and re-invented multiple times which led to me making some Design Choices such as: Separation between prod and lab Single ingress point Minimal public attack surface Some of things which I would eventually like to perfect in this setup include: Strong identity and access control Reproducible infrastructure Avoiding Single Points of Failure Locations / Tenancies On-Prem Homelab Due to a single High end hardware and 3 low spec devices I decided to divide Prod and Test into 2 mostly-independent environments former being an Independent PVE and PBS node and latter being a PVE Cluster to learn on Host CPU RAM Environment PC Ryzen 5 5600x 48 GB DDR4 Prod Pi-4 Cortex-A72 4 GB LPDDR4 Test Acer AMD A8-7410 16 GB DDR3 Test HP i3-7020U 32 GB DDR4 Test Oracle Cloud Infrastructure (OCI) This tenancy has high Uptime by the virtue of it being “Cloud” thus it has been crowned as the INGRESS Currently I am only effective using One ARM machine but I do have plans to utilize 3 VMs here to mitigate single point of failure to an extent. Keep in tune for that blog! Instance Shape Arch Environment OCI-Bom-ARM VM.Standard.A1.Flex ARM Prod OCI-Bom-AMD-1 VM.Standard.E2.1.Micro x86 Test OCI-Bom-AMD-2 VM.Standard.E2.1.Micro x86 Test The two VMs in Test environment are ephemeral Virtualization Stack Why Proxmox Proxmox is quite a popular solution in the Selfhosting community which translates to better support. I also found it easier to get started with Proxmox with this support. Existence of PBS and PVE Helper Scripts now reinforces the decision of deploying PVE Since Proxmox is based on KVM, guides lining out Hardware Passthrough work almost 1:1 on Proxmox. Underlying is a Debian Environment which makes it easier to debug, maintain and troubleshoot for a regular Linux user like me VM vs LXC vs Docker VMs are divided based on their usage such as Media for Media Server, k3s for VMs running Kubernetes Cluster etc. VMs are first-class citizen in the what-to-use debate in my Homelab. LXCs are used when I come across hardware constraints, such as, Single GPU Passthrough, SATA Passthrough when SATA bus is shared with USB bus in the motherboard For specific service deployment such as DNS, Reverse Proxy etc, Docker is preferred unless using Docker would require additional un-necessary configuration or more importantly, additional Security Permissions which is the case when it comes to DNS Both LXCs and Docker deploy services based on stacks such as DNS Stack which consists of Pihole, Unbound and Pihole-Updatelist. This way ensures I don’t over-complicate my infra but still retain necessary segregation High-Level Architecture Explanation Let’s assume a request is sent to dash.example.com which corresponds to service: dash on Other-Services stack in Media Server at Proxmox PC On-Prem The request will only reach Traefik at OCI-Bom-Arm. Traefik will check it’s backend i.e, File and Redis. If service is found, it will redirect to Keycloak and authenticated via: Native Auth if : service supports OAuth and OAuth is setup at service Middleware Auth if: no native OAuth is setup AND oauth middleware is applied at service But how does Traefik know where Dash service is at? This is where Redis backend and Tailscale come to the picture! Traefik-Kop is a service which writes to Redis over at OCI-Bom-ARM and traffic is routed using Headscale and Outbound NAT at OPNsense Thanks! Thank you for going through this long, complicated story of my Homelab, do leave your comment, react and share this with your Tech Ninja Friends!" } ]
